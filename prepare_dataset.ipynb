{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "import torch\n",
    "from transformers import PreTrainedTokenizer, PreTrainedTokenizerFast, AutoTokenizer\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "# HuggingFace hardcodes the ignore index to -100\n",
    "_HF_IGNORE_INDEX = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'vilm/vinallama-7b-chat'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, \n",
    "                                          trust_remote_code=True, \n",
    "                                          token='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',\n",
    "                                          cache_dir='../cache',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = tokenizer.get_vocab()\n",
    "# import json \n",
    "# with open('tokenizer.json', 'w') as f:\n",
    "#     json.dump(vocab, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VinaLLaMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.chat_template = (\n",
    "    \"{% for message in messages %}\"\n",
    "    \"{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"{% if add_generation_prompt %}\"\n",
    "    \"{{ '<|im_start|>assistant\\n' }}\"\n",
    "    \"{% endif %}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_prompt(question, context=None):\n",
    "    if context is None:\n",
    "        return question\n",
    "    else:\n",
    "        return f'### Đây là những thông tin liên quan:\\n{context}\\n### Hãy trả lời câu hỏi:\\n{question}'\n",
    "\n",
    "# def format_prompt(sample):\n",
    "#     context, question = sample['context'], sample['question']\n",
    "#     human_prompt = gen_prompt(question, context)\n",
    "\n",
    "#     system =  \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\"\n",
    "\n",
    "#     return f\"{system} USER: {human_prompt} ASSISTANT:\", sample['answer']\n",
    "def format_prompt_vinallama(sample):\n",
    "    context, question = sample['context'], sample['question']\n",
    "\n",
    "    prompt = gen_prompt(question, context)\n",
    "    system = 'Bạn là một trợ lí AI hữu ích. Hãy trả lời người dùng một cách chính xác.\\n' \n",
    "    messages = [{'role': 'system', 'content': system}, \n",
    "                {'role': 'user', 'content': prompt},]\n",
    "    \n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True), sample['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def template_data(sample): \n",
    "    prompt, response = format_prompt_vinallama(sample)\n",
    "    sample['prompt'] = prompt\n",
    "    sample['response'] = response\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(sample):\n",
    "    try:\n",
    "        tokenizer(text=sample['prompt'], text_target=sample['response'])\n",
    "    except:\n",
    "        print('-----------------')\n",
    "        print(sample['prompt'])\n",
    "        print(sample['response'])\n",
    "    return tokenizer(text=sample['prompt'], text_target=sample['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_list(x: Union[List, torch.Tensor]) -> List:\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        x = list(x.flatten())\n",
    "    assert isinstance(x, list)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_and_batch_decoder_only(examples, max_seq_len=2048):\n",
    "    # examples: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "    # Steps explained in comments\n",
    "\n",
    "    processed_examples = []\n",
    "    for context, target in zip(examples['input_ids'], examples['labels']):\n",
    "        # context = ensure_list(example['input_ids'])\n",
    "        # target = ensure_list(example['labels'])\n",
    "\n",
    "        context = ensure_list(context)\n",
    "        target = ensure_list(target)\n",
    "        # First, get rid of any padding tokens\n",
    "        context = [t for t in context if t != tokenizer.pad_token_id]\n",
    "        target = [t for t in target if t != tokenizer.pad_token_id]\n",
    "\n",
    "        # Third, ensure that the target text ends with an eos tag\n",
    "        if target[-1] != tokenizer.eos_token_id:\n",
    "            target = target + [tokenizer.eos_token_id]\n",
    "\n",
    "        n_context = len(context)\n",
    "        n_target = len(target)\n",
    "\n",
    "        # if n_context >= max_seq_len:\n",
    "        #     warnings.warn(\n",
    "        #         f'Skipping example because CONTEXT length={n_context} leaves no room ' +\\\n",
    "        #         f'for TARGET tokens because max_seq_len={max_seq_len}. ' +\\\n",
    "        #         f'If this causes downstream issues because of inconsistent batch sizes, ' +\\\n",
    "        #         f'consider increasing max_seq_len or using example packing.'\n",
    "        #     )\n",
    "        #     continue\n",
    "\n",
    "        # We need to concatenate the context and target to get the\n",
    "        # full input sequence, cutting off any excess tokens from the\n",
    "        # end of the target\n",
    "        # if n_context + n_target > max_seq_len:\n",
    "        #     old_n_target = int(n_target)\n",
    "        #     n_target = max_seq_len - n_context\n",
    "        #     warnings.warn(\n",
    "        #         f'Truncating TARGET sequence of length={old_n_target} to length={n_target}, ' +\\\n",
    "        #         f'so context+target fit max_seq_len={max_seq_len}. If truncation is ' +\\\n",
    "        #         f'a problem, consider increasing max_seq_len.')\n",
    "        #     target = target[-n_target:]\n",
    "        #     target[-1] = tokenizer.eos_token_id\n",
    "        if n_context + n_target >= max_seq_len:\n",
    "            warnings.warn(\n",
    "                f'Skipping example, total length of context and target is {n_context + n_target}')\n",
    "            continue\n",
    "        n_total = n_context + n_target\n",
    "\n",
    "        input_ids = context + target\n",
    "        labels = ([_HF_IGNORE_INDEX] * n_context) + target\n",
    "        attention_mask = [1] * n_total\n",
    "        # bidirectional_mask is used by our prefix lm model variants\n",
    "        # bidirectional_mask = ([1] * n_context) + ([0] * n_target)\n",
    "\n",
    "        # Annoyingly, we need to pad the everything but input_ids\n",
    "        # and attention_mask ourselves\n",
    "        i_pad = [_HF_IGNORE_INDEX] * (max_seq_len - n_total)\n",
    "        # z_pad = [0] * (max_seq_len - n_total)\n",
    "        if tokenizer.padding_side == 'left':\n",
    "            labels = i_pad + labels\n",
    "            # bidirectional_mask = z_pad + bidirectional_mask\n",
    "        else:\n",
    "            labels = labels + i_pad\n",
    "            # bidirectional_mask = bidirectional_mask + z_pad\n",
    "\n",
    "        # Update the example\n",
    "        example = {}\n",
    "        example['input_ids'] = input_ids\n",
    "        example['labels'] = labels\n",
    "        example['attention_mask'] = attention_mask\n",
    "        # example['bidirectional_mask'] = bidirectional_mask\n",
    "\n",
    "        processed_examples.append(example)\n",
    "\n",
    "    batch = tokenizer.pad(\n",
    "        processed_examples,\n",
    "        padding='max_length',\n",
    "        max_length=max_seq_len,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset \n",
    "import os \n",
    "# dataset = load_dataset('csv', data_files='/home4/tuannd/llm-training/Data_Vi_QA_v1.1/QA_Uni/Chitchat_HUST_train.csv', split='train')\n",
    "# dataset[0]\n",
    "import pandas as pd\n",
    "\n",
    "# data_dir = '/home4/tuannd/llm-training/data/train_v2/final'\n",
    "# data_files = os.listdir(data_dir)\n",
    "# # data_files = ['/home4/tuannd/llm-training/data/Data_Vi_QA_v1.1/QA_Uni/Chitchat_HUST_train.csv',\n",
    "# #               '/home4/tuannd/llm-training/data/Data_Vi_QA_v1.1/QA_Uni/hust_no_ans.csv',\n",
    "# #               '/home4/tuannd/llm-training/data/Data_Vi_QA_v1.1/QA_Uni/Uni-QA(08_12_2023).csv']\n",
    "# print(data_files)\n",
    "# all_df = pd.concat([pd.read_csv(data_dir + '/' + f) for f in data_files])\n",
    "# all_df = all_df.dropna(subset=['answer'])\n",
    "all_df = pd.read_csv('/home4/tuannd/llm-training/data/train_v2/final/train_v2.csv')\n",
    "# len(all_df)\n",
    "dataset = Dataset.from_pandas(all_df)\n",
    "dataset = dataset.remove_columns(['type'])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: len(x['answer'].split()) > 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = dataset.map(template_data, remove_columns=['question', 'context', 'answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "i = random.randint(0, len(processed_dataset))\n",
    "print(processed_dataset[i]['prompt'])\n",
    "print('-----------------')\n",
    "print(processed_dataset[i]['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = processed_dataset.map(tokenize_data, batched=False, remove_columns=['prompt', 'response'])\n",
    "# tokenized_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_LENGTH = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenized_dataset.filter(lambda x: len(x['input_ids']) + len(x['labels']) + 2 <= CONTEXT_LENGTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stats of length of input_ids + labels\n",
    "lengths = []\n",
    "for example in tokenized_dataset:\n",
    "    lengths.append(len(example['input_ids']) + len(example['labels']))\n",
    "\n",
    "import numpy as np\n",
    "np.mean(lengths), np.std(lengths), np.max(lengths), np.min(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.filter(lambda x: len(x['input_ids']) + len(x['labels']) + 2 < CONTEXT_LENGTH)\n",
    "len(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.pad_token_id = 3\n",
    "print(tokenizer.pad_token)\n",
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "final_dataset = tokenized_dataset.map(partial(_process_and_batch_decoder_only, max_seq_len=CONTEXT_LENGTH), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset.save_to_disk('/home4/tuannd/llm-training/viqauni_vinallama_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = random.randint(0, len(final_dataset))\n",
    "print(tokenizer.decode(final_dataset[i]['input_ids'], skip_special_tokens=False))\n",
    "\n",
    "labels = [i for i in final_dataset[i]['labels'] if i != -100]\n",
    "tokenizer.decode(labels, skip_special_tokens=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
